app {

  spark {
    name = "Batch Producer"
//    master = "local[1]"
    master = "spark://7f3a14c8d5db:7077"
    enableHiveSupport = true
    properties {
//      spark.hadoop.hive.metastore.warehouse.dir = "hdfs://namenode:8020/user/hive/warehouse"
//      javax.jdo.option.ConnectionURL= "jdbc:mysql://127.0.0.1:3306/metastore_v1"
//      javax.jdo.option.ConnectionDriverName= "com.mysql.cj.jdbc.Driver"
//      javax.jdo.option.ConnectionURL= "jdbc:postgresql://127.0.0.1/metastore"
//      javax.jdo.option.ConnectionDriverName= "org.postgresql.Driver"
//      javax.jdo.option.ConnectionPassword= "hive"
//      javax.jdo.option.ConnectionUserName= "hive"
      hive.metastore.uris = "thrift://127.0.0.1:9083"
    }
  }

  batch {
    interval = "day"
    source = "hive"
    sink = "kafka"

    // date format pattern : "yyyy-MM-dd[ [HH][:mm][:ss][.SSS]]"
    from = "2020-01-01"
    to = "2020-01-01"
  }

  source_hive {
    table = "test.demo"
    // used by java.time.format.DateTimeFormatter.ofPattern(java.lang.String)
    // ex) "'year =' '' yyyy '' 'and month =' '' MM '' 'and day =' '' dd ''" is converted to "year = '2020' and month = '01' and day = '01'"
    partition_pattern = "'year =' ''yyyy'' 'and month =' ''MM'' 'and day =' ''dd'' "
  }

  sink_kafka {
    servers = [ "10.205.72.189:9092" ]
    topic = "test"
    format = "csv"
    separator = ","
    properties {
      kafka.compression.type = "lz4"
      kafka.max.request.size = "1048576000"
    }
  }

}